<!--
Compile :
    pandoc -f markdown notes/somefile.md - -filter pandoc-crossref -t latex -o somefile.pdf

Notes:
    1. http://lierdakil.github.io/pandoc-crossref/
    #. On over/under braces : https://tex.stackexchange.com/a/132527/84495
-->


<!--
    YAML section
-->
---
title: Notes on Ch. 2 in Elements of Statistical Learning
author: Ali Snedden
date: 2021-12-07
abstract:
...
---
header-includes:
  - \hypersetup{colorlinks=true,
            urlcolor=blue,
            pdfborderstyle={/S/U/W 1}}
    \usepackage{mathtools}
---
\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\colorbox{codegray}{\texttt{#1}}}
<!-- \let\overbracket\overbracket[0.2mm][0.6mm]{#1}      not sure that this works -->

\maketitle
\tableofcontents
\pagebreak


1. Read most of this chapter w/o taking notes in markdown, add notes later

Jargon
==========================
1. Recall Hastie uses $\hat{f}$ to denote predictions

2.1 Introduction
==========================

2.2 Variable Types and Terminology
==========================

2.3 Two Simple Approaches to Prediction, Least Squares and Nearest Neighbors
==========================
1. Linear Models and Least Squares
    a) Given vector of inputs, $X^{T} = (X_{1}, X_{2}, ..., X_{p})$, predicted output is:
       $$ \hat{Y} = \hat{\beta_{0}} + \sum^{p}_{j=1} X_{j} \hat{\beta}_{j}$$    {#eq:2.1}
    #) $\hat{\beta}$ is \emph{bias} in ML
    #) Transform $X^{T}$ to $X^{T} = (1, X_{1}, X_{2}, ..., X_{p})$, so we can package
       $\hat{\beta}_{0}$ into $\beta$ (could use Einstein notation here...)
       $$ \hat{Y} = X \hat{\beta}$$                                             {#eq:2.2}
    #) Usually, $\hat{Y}$ is a scalar, but generallizing : 
        #. $\hat{Y}$ can be a $K$-vector, $\beta$ would be $p \times K$ Matrix and 
    #) Least Squared fitting (most popular)
        $$RSS(\beta) = \sum^{N}_{i=1}(y_{i} - x_{i}^{T}\beta)^{2}$$             {#eq:2.3}
        Rewrite in matrix notation and do dimentional analysis
        $$RSS(\beta) = (\underbracket[0.2mm][0.6mm]{\bf y}_{\text{N}} - \overbracket[0.2mm][0.6mm]{\bf X}^{N\times p} \underbracket[0.2mm][0.6mm]{\beta}_{p})^{T}({\bf y} - {\bf X}\beta) $$  {#eq:2.4}
        Optimizing for $\beta$ (i.e. taking the derivative w/r/t $\beta$)
        $$ \frac{d}{d \beta} \big( RSS(\beta) = ({\bf y} - {\bf X} \beta)^{T}({\bf y} - {\bf X}\beta)\big) = 0$$ }
        That is hard, let's do a simple substitution, let $\beta' = ({\bf y} - {\bf X} \beta)$.
        Now above eqn becomes
        $$ \frac{d}{d \beta} \big( \beta'^{T} \beta' \big)= 2 \beta'^{T} \frac{d \beta'}{d \beta} = 0 $$ 
        Now substitute in for $\beta'$
        $$ \frac{d}{d \beta} \big( \beta'^{T} \beta' \big) = 2({\bf y} - {\bf X}\beta)^{T} \frac{d ({\bf y} - {\bf X}\beta)}{d \beta} = 2({\bf y} - {\bf X}\beta)^{T}(-{\bf X}) = 0$$ 
        Cancel -2 factor and take transpose across both sides 
        $$ {\bf X}^{T}({\bf y} - {\bf X}\beta) = 0$$                            {#eq:2.5}
        Now solve for $\beta$
        $$ {\bf X}^{T} {\bf y} - {\bf X}^{T} {\bf X}\beta = 0$$
        $$  + {\bf X}^{T} {\bf X}\beta = + {\bf X}^{T} {\bf y}$$
        $$   ({\bf X}^{T} {\bf X})^{-1} \big[{\bf X}^{T} {\bf X}\beta =  {\bf X}^{T} {\bf y} \big]$$
        $$   \beta =  ({\bf X}^{T} {\bf X})^{-1} {\bf X}^{T} {\bf y} \big]$$    {#eq:2.6}
    #) In Hasties' Figure 2.1, he uses categorical data where the points are orange or blue
        #. Blue : $Y = 0$ 
        #. Orange : $Y = 1$
    #) For the prediction, he cuts on $\hat{Y} = 0.5$, so 
        $$ \hat{G} = \left\{ \begin{array}{lr}
                                    \text{Orange} & \text{if  } \hat{Y} > 0.5 \\
                                    \text{Blue  } & \text{if  } \hat{Y}\leq 0.5
                                \end{array}
                       \right.
           $$     {#eq:2.7}
    #) The linear fit isn't perfect. Consider two possibilities
        #. Scenario 1 : The training data were generated by bivariate gaussian
                        distributions with uncorrelated components and different means
        #. Scenario 2 : The training data in each class came from a mixture of 10 
                        low-variance Gaussian distributions, with individual means dist. as
                        a gaussian.
        #. He has yet to tell us which it is.
#. Nearest-neighbor methods (2.3.2)
    a) The $k$-nearest neighbor fit (where $N_{k}(x)$ is neighborhood of $x$ with $k$
       closest points.
        $$ \hat{Y}(x) = \frac{1}{k} \sum_{x_{i} \in N_{k}(x)} y_{i}$$    {#eq:2.8}
    #) See \code{nearest\_neighbor.py} for implementation details
    #) When $k$ = 1, it corresponds to a \emph{Voronoi tessellation}
        #. NONE of the points are misclassified
        #. It is grossly overfitted
    #) Can't use minimizing errors (like least squares) to pick $k$. It would always pick   
       $k$ = 1
#. From Least Squares to Nearest Neighbors  (2.3.3)
    a) Least Squares fit = Low  Variance,   (potentially) high bias
        #. More suitable for Scenario 1
    #) Nearest Neighbors = High Varieance,  low bias
        #. More suitable for Scenario 2
    #) Data generated from
        #. BLUE : 10 means from bivariate Gassian distribution $N((1,0)^{T}, {\bf I})$
        #. ORANGE : 10 means from bivariate Gassian distribution $N((0,1)^{T}, {\bf I})$
        #. Then for BLUE (ORANGE) a mean was randomely selected from one of the 10 above 
           means ($m_{k}$), a 1/10 chance
        #. Then a point is selected from $N(m_{k}, {\bf I}/5)$
    #) Modifications and improvements
        #. Kernel methods use weights that decrease smoothly to zero from target point
           instead of the 0/1 effective weights used by $k$-nearest neighbors
        #. In high-dimensional spaces, the distance kernels are modified to emphasize
           some variable more than others
        #. Local regression fits linear models by locally weighted least squares rather 
           than fitting constants locally
        #. Linear models fit to a basis expansion of the original inputs allow 
           arbitrarily complex models
        #. Projection pursuit and neural network models consit of sums of non-linearly 
           transformed linear models.


2.4 Statistical Decision Theory
==========================
1. Background
    a) Conditional probability
        #. Given like $\text{Pr}(Y|X)$ 
        #. Said like "Probability of $Y$ given $X$"
        #. Example 1 : (from wiki), 
            * The probability of anyone having a cough is 
                $$\text{Pr}(cough) = 5%$$
              but, if a person is sick, that percentage may go up significantly, e.g.
                $$\text{Pr}(cough|sick) = 75%$$
        #. $\text{Pr}(Y|X)$ = Conditional Probility
        #. $\text{Pr}(Y)$   = Unconditional Probility
        #. If $\text{Pr}(Y|X) = \text{Pr}(Y)$ then $X$ and $Y$ are independent
        #. Example 2 : (from wiki), 
            * The probability of testing positive ($A$) if infected with dengue ($B$) is 90%, 
                $$\text{Pr}(A|B) = 90%$$
            * However there are high false positive rates in the dengue test, only 15% of
              positive tests are from people with dengue, ie
                $$\text{Pr}(B|A) = 15%$$
    #) Bayes' theorem
        $$\text{Pr}(A|B) = \frac{\text{Pr}(B|A) \text{Pr}(A)}{\text{B}}$$
        
            
#. Consider variables : 
    a) $X \in \mathbb{R}^{p}$ be a random input vector. 
    #) $Y \in \mathbb{R}$ be the random output variable.  
    #) $X$ and $Y$ are related by the joint probability distribution, $\text{Pr}(X,Y)$
    #) Want a function $f(X)$ to predict $\hat{Y}$ given $X$
    #) Introduce \emph{loss function} $L(Y,f(X)) = (Y - f(X))^{2}$ to penalize prediction
       error
    #) The Expected Prediction Error (EPE)
        $$ \text{EPE}(f) = \text{E}(Y - f(X))^{2}$$                             {#eq:2.9}
        $$ \text{EPE}(f) = \int [y - f(x)]^{2}\text{Pr}(dx,dy)$$                {#eq:2.10}
    #) Conditionnig, i.e. factoring 
        $$\text{Pr}(X,Y) = \text{Pr}(Y|X)\text{Pr}(X)$$
       where
        $$\text{Pr}(Y|X) = \text{Pr}(Y,X)/\text{Pr}(X)$$
    #) Using this in eqn \ref{eq:2.10}
        $$ \text{EPE}(f) = \int [y - f(x)]^{2}\text{Pr}(dy|dx)\text{Pr}(dx)$$  
       some magic...I can understand or sort of see how you get expectation values, but still
       the math needs explained to me.
        $$ \text{EPE}(f) = \text{E}_{X} \text{E}_{Y|X} ([Y-f(X)]^{2}|X)$$       {#eq:2.11}
    #) Once again, we want to minimize eqn \ref{eq:2.11} to get $f(x)$
        $$ f(x) = \text{argmin}_{c} \text{E}_{Y|X} ([Y-c]^{2}|X=x)$$            {#eq:2.12}
       where $X=x$ is the conditional mean
    #) Solution is 
        $$ f(x) = \text{E}(Y|X=x)$$                                             {#eq:2.13}
    #) Can ask for the average of all the $y_{i}$'s with input $x_{i} = x$, but usually
       there is only one $y_{i}$ for each $x_{i}$
        $$ f(x) = \text{Average}(y_{i}|x_{i} \in N_{k}(x))$$                    {#eq:2.14}
       where $N_{k}(x)$ is the neighborhood containing $k$ points in $T$ closest to x.
       Some approximations : 
        #. Expectation is approx by averaging over sample data
        #. Conditioning (in sense of conditional probability) at a point is relaxed to 
           conditioning on some region "close" to the target point
    #) QUESTION : How do I get from eqns (\ref{eq:2.11} = \ref{eq:2.14})?
    #) Consider eqn \ref{eq:2.14} as $N$ (and $k$) go large. 
        #. $k/N \rightarrow 0$
        #. $\hat{f}(x) \rightarrow \text{E}(Y|X=x)$
        #. Issues
            * As dimensionality goes big, the rate of convergence goes to hell
            * We need to determine $\text{E}(Y|X)$ frome the data, 
    #) Consider linear regression w/in this framework
        
    

    


#. Questions
    a) $E_{x}$ = expectation value?
        #. I think so
    #) I don't understand the notation in eqn \ref{eq:2.10}
    #) I don't understand the math going from eqn \ref{eq:2.10} to eqn \ref{eq:2.11}?
       How do I handle dealing with differentials in joint probability distribution
       functions?
    #) How do we go from eqn \ref{eq:2.12} to eqn \ref{eq:2.13}
    



2.5 Local Methods in High Dimensions
==========================

2.6 Statistical Models, Supervised Learning and Function Approximation
==========================
1. Review
    a) Goal
        #. Find approx of $\hat{f}(x)$ to underlying function $f(x)$
    #) Previously
        #. 
    
#. A Statistical Model for the Joint Distribution Pr(X, Y)
